---
title: "Class09_miniproject"
author: "Byanca Lima Valenzuela (A16688058)"
date: "10/26/2021"
output: pdf_document
---
#Preparing the data
#Save your input data file into your Project directory

```{r}
fna.data<- "WisconsinCancer.csv"
```

#Complete the following code to input the data and store as wisc.df

```{r}
wisc.df<-read.csv(fna.data, row.names=1)
```

```{r}
head(wisc.df)
```

#We can use -1 here to remove the first column

```{r}
dim(wisc.df)
```


```{r}
wisc.data=as.matrix(wisc.df[,2:31])
head(wisc.data)
```

```{r}
head(wisc.data)
```
#Create dianogsis vector for later

```{r}
diagnosis<- as.factor(wisc.df$diagnosis)
```

#Exploring data analysis

>Q1. How many observations are in this dataset?
569


>Q2. How many M and B samples are there?
        B: 357  M:212

```{r}
table(diagnosis)
```

>Q3. How many col have the suffic "mean"?
          10

```{r}
length(grep("mean", colnames(wisc.df)))
```

2. Principal Component Analysis 

#Performing PCA

#Check column means and standard deviations

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

#Perform PCA on wisc.data by completing the following code

```{r}
wisc.pr <- prcomp(wisc.data, scale =T)
summary(wisc.pr)
```

#Look at summary of results

```{r}
summary(wisc.pr)
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
        0.4427
>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
          3 

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
          7

#Interpreting PCA results

```{r}
biplot(wisc.pr)
```
>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
It is using rownames as the ploting character so it is diffucult to read. 

#Scatter plot observations by components 1 and 2

```{r}
plot(wisc.pr$x, col=diagnosis, xlab= "PC1", ylab="PC2")
```

#Another way to plot

```{r}
plot(wisc.pr$x[,1:2],col=diagnosis)
```

#Repeat for components 1 and 3

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3],col=diagnosis,
     xlab="PC1", ylab="PC3")
```
#Another way to plot

```{r}
plot(wisc.pr$x[,1:3], col=diagnosis)
```
>Q8. What do you notice about these plots?
It separates the data into two subgroups. 

#Create a data.frame for ggplot

```{r}
df<- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis 
```

#Load the ggplot2 package

```{r}
library(ggplot2)
```

#Make a scatter plot colored by diagnosis 

```{r}
ggplot(df) + 
  aes (PC1, PC2, col= diagnosis)+
  geom_point()
```

          
# Variance explained 
```{r}
pr.var<- wisc.pr$sdev^2
head(pr.var)
```
# Variance explained by each principal component: pve

```{r}
pve <- pr.var / sum(pr.var)
pve
```


# Plot variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

# Alternative scree plot of the same data, note data driven y-axis
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


```{r}
summary(wisc.pr)
```

#Communicatign PCA results

>Q9.For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
        -0.2608538

```{r}
wisc.pr$rotation["concave.points_mean",1]
```
>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
          4
          
```{r}
var<- summary (wisc.pr)
sum(var$importance[3,] < 0.8)
```

#3. Hierarchical clustering

        
#Scale the wisc.data data using the "scale()" function


```{r}
data.scaled<- scale(wisc.data)
data.dist<- dist(data.scaled)
wisc.pc.hclust<-hclust(data.dist)
plot(wisc.pc.hclust)
```

#Results of hiertarchical clustering 
>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
        5

```{r}
plot(wisc.pc.hclust)
abline(h=4, col="red", lty=2)
```

#Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.pc.hclust, k=4)
```

```{r}
table(wisc.hclust.clusters,diagnosis)
```

>Q12. Can you find a better cluster vs. diagnoses match by cutting into a different number of clusters between 2 and 10?

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning. 
            ward.D2

#5. Combining methods
We take the results of our PCA analysis and cluster in this space 'wisc.pr$x'

```{r}
summary(wisc.pr)
```

#Cut the tree into k=2 groups

```{r}
grps<-cutree(wisc.pc.hclust, k=2)
table(grps)
```

Cross table 

```{r}
table(diagnosis, grps)
```

#Use the distance along the first 7 Pcs for clustering 

```{r}
wisc.pc.hclust<-hclust(dist(wisc.pr$x[,1:3]), method="ward.D2")
```

```{r}
wisc.pc.hclust.clusters<- cutree(wisc.pc.hclust, k=2)
```

#Plot my dendrogram

```{r}
plot(wisc.pc.hclust)
abline(h=60, col="red")
```

#Compare to actual diagnosis

```{r}
table(wisc.pc.hclust.clusters, diagnosis)
```
>Q15. How well does the newly created model with 4 clusters separate out the two diagnoses?


>Q16. Cannot answer since I did not do the optional assignment (4)

#6. Sensivity/Specificity

>Q17. Which of your analysis precedures resulted in a clustering model with the best specfificty and sensitivity? 

Accuracy 

```{r}
(333+179)/nrow(wisc.data)
```

Sensitivity

```{r}
(179/(179+333))
```

Specificity

```{r}
333/(333+24)
```

#7. Prediction
#url <- "new_samples.csv"

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q18.Which of these new patients should we prioritize for follow up based on your results?
          2
