---
title: "Machine Learning 1"
author: "Byanca (16688058)"
date: "10/21/2021"
output: html_document
---

First up is clustering methods

# Kmeans clustering

The function in base R to do kmeans clustering is called 'kmeans()'

First make up some data where we know what the answer should be: 

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, 3))
x <- cbind(x=tmp, y =rev(tmp))
plot(x)
```

> Q. Can we use kmeans() to cluster this data setting k to 2 and nstart 20?

```{r}
km <- kmeans(x, centers = 2, nstart=20)
km
```

> Q. How many points are in each cluster?

```{r}
km$size
```

> Q. What component of your result object details cluster assignment/membership?

```{r}
km$cluster
```


> Q. Cluster center?

```{r}
km$centers
```
> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x, col=km$cluster) 
points(km$centers, col="blue", pch=15, cex=2)
```


# Hierarchial Clustering

A big limitation with k-means is that we have to tell it K ( the number of clusters we want).


Analyze this same data with hclust()

Demonstrate the used of dist(), hclust(), plot(), and cutree() function to do clustering.
Generate dendrograms and return cluster assignment and membership vector...

```{r}
hc <- hclust(dist(x))
hc
```

There is a plot method for hclust result objecs. Let's see it.

```{r}
plot(hc)
```

To get our cluster membership vector, we have to do a wee bit more work. We have to "cut" the tree where we think it makes sense. For this, we use the 'cutree()' function. 

```{r}
cutree(hc, h= 6)
```

You can also call 'cutree()' setting k= the number of grps/clusters you want. 

```{r}
grps <- cutree(hc, k=2)
```

Make our results plot

```{r}
plot(x, col=grps)
```






# Principal Component Analysis

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

```{r}
dim(x)
```
> Q1. There are 17 rows and 4 columns. R functions needed are : dim(x), ncol(x), nrow(x)

# Note how the minus indexing works

```{r}
x <- read.csv(url, row.names=1)
head(x)
```

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
head(x)
```

> Q2. I prefer the first solving because it gives me a better visual understanding. The first is more robust because it deletes columns as you run the code. 

Now we have the data looking good we want to explore it. We will use some conventional plot (barplots & pair plots)

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
> Q3. Changing the 'besides()' argument results the following plot. 

The stacked barplot is not helpful. 


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

```

> Q5. 
If a given point lies on the diaginal for a given plot it means that it is following the expected trend. 

```{r}
pairs(x, col=rainbow(10), pch=16)
```
> Q6. 

# PCA to the rescue!

The main function is base R for PCA is 'prcomp().
This want's the transpose of our data

```{r}
pca <- prcomp(t(x))
  summary (pca)
```

```{r}
attributes(pca)
```


```{r}
plot(pca$x[,1], pca$x[,2])
```

# Plot PC1 vs PC2

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
colors<-c("orange", "red", "blue", "green")
text(pca$x[,1], pca$x[,2], colnames(x), col=colors)
```


```{r}
v <- round (pca$sdev^2/sum(pca$sdev^2)*100)
v
```

## or the second row here 

```{r}
z <- summary (pca)
z$importance 
```

```{r}
barplot(v, xlab= "Principal Component", ylab="Percent Variation")
```

## Lets focus on PC1 as it accounts for > 90% of variance

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot(pca$rotation[,1], las=2)
```
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot(pca$rotation[,2], las=2)
```

> Q9. What two food groupd feature promonantely and what does PC2 mainly tell us about?
Alcooholic drinks and fresh potatoes. 

```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

> Q10. How many genes and samples are in this data?
10 samples and 6 genes

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca)
```

```{r}
plot(pca, main="Quick scree plot")
```
## Variance captured per PC 

```{r}
pca.var <- pca$sdev^2
```





## Percent variance is often more informative to look at 

```{r}
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
barplot(pca.var.per, main="Screen Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

## A vector of colors for wt and ko samples

```{r}
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))

```

Using ggplot

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)

# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```

# Add a 'wt' and 'ko' "condition" column

```{r}
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE)

```

Add some spit and polish

```{r}
p + labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clealy seperates wild-type from knock-out samples",
       x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="BIMM143 example data") +
     theme_bw()

```

